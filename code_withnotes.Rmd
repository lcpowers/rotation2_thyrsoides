---
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### **Time-Lagged Integral Projection Model**

### **Study species:** *Campanula thyrsoides*

### **Last update:** Sunday, 13.07.2007

---

### Required input files

1. "ct.ipm.txt"   
columns: 	n == Unique number for each individual    
		site == Population abbreviation (here: FU == Furka Pass, SP == Schynige Platte)   
		year.t == here: 2004 or 2005    
		nl.t == number of rosette leaves in year t    
		nl.t1 == number of rosette leaves in year t+1   
		ll.t == length of longest rosette leave in year t   
		ll.t1 == length of longest rosette leave in year t+1    
		surv == survival: 1 == yes, 0 == no   
		flow == flowering: 1 == yes, 0 == no    
		ros == number of rosettes in individual   

2. "ct.IPM.fecundity.txt"   
columns: 	n == Unique number for each individual    
		site == Population abbreviation (here: FU == Furka Pass, SP == Schynige Platte)   
   	year.t == here: 2004 or 2005    
		ll.t == length of longest rosette leave in year t   
		nl.t == number of rosette leaves in year t    
		si.t1 == seeds per individual in year t+1   
		brows.t1 == browsed in year t+1 (here: 1 ==Yes, 0 == No)    
		ros == number of rosettes in individual   
		size.t == size in year t    

3. "CT.IPM.seedlings.txt"   
columns:	n == Unique number for each individual   
		site ==  Population abbreviation (here: FU == Furka Pass, SP == Schynige Platte)   
		year.t == here: 2004 or 2005   
		ll.t ==  length of longest rosette leave in year t   
		nl.t ==  number of rosette leaves in year t   
		size.t == size in year t    

4. "IPM.establishment.data.txt"   
columns:	site == Population abbreviation (here: FU == Furka Pass, SP == Schynige Platte)   
 		plot == Number of plot    
		year.t == here: 2004 or 2005    
		seeds.t == seed production in each plot   
		sdl.t1 == seedlings in year t+1 in each plot    

5. "ipm.saf.data.txt"   
 columns:	n == Unique number for each individual    
		site == Population abbreviation (here: FU == Furka Pass, SP == Schynige Platte)   
		flo == Flowering (here: 1==Yes, 0==NO)    
		age == Age 2005   

---
#### Load packages
```{r packages, message=FALSE}
# Clear environment
rm(list=ls())

# Load packages
library(tidyverse)
library(nlme)
library(MASS)
library(cowplot)

setwd("~/Desktop/IQ/Rotation2/rotation2_thyrsoides")

# Detach any lingering attached datafs
while(any(search()=="dataf_r1")) detach(dataf_r1)
```


## Part I. Fitting Models
#### Read first in dataset
```{r reading_in_data}

dataf = data.frame(read.table("ct.ipm.txt",header=T))
# names(dataf)

# attach(dataf) ## Set reference dataframe

# Filter full dataframe for plants with 1 rosette
dataf_r1 = filter(dataf,ros==1)
attach(dataf_r1)

dataf_r1$size.t.all = log(nl.t*ll.t) ## plant sizes in year t

dataf_r1$size.t1.all = log(nl.t1*ll.t1) ## plant sizes in year t+1

## The next three are unnecessary since they are just duplicating data in another column, but keeping for consistency at this point
dataf_r1$flow.all = flow ## Logical vector if flowering: 1 = yes
dataf_r1$surv.all = surv ## Logical vector of survival: 1 = yes
dataf_r1$site.all = site ## Site vector

site.code.l=c("FU","SP") ## Used later for plots
pch.code=c(19,1)

# Detach old version, attach new version
detach(dataf_r1)
attach(dataf_r1)

all.sizes=c(size.t.all[flow.all==0],size.t1.all[year.t==2005]) ## all plant sizes
all.site=c(site.all[flow.all==0],site.all[year.t==2005]) ## All sites -- how is this one used?
detach(dataf_r1)
```

#### Calculation: Growth
```{r calc_growth}

growth_df <- dataf_r1 %>% 
  filter(flow.all==0) %>% 
  dplyr::select(size.t = size.t.all,
                size.t1 = size.t1.all,
                site.s = site.all) %>% 
  filter(complete.cases(.))

######### check whether variance structure is needed #########
## AKA check for heterogeneity?
fit.grow.gls.1<-gls(size.t1~size.t+site.s,
                    na.action=na.omit,
                    weight=varExp(form=~fitted(.)|site.s), ## Exponential of the variance covariate grouped at site level?
                    method="ML",
                    data=growth_df); ## Maximum likelihood
# summary(fit.grow.gls.1)
# plot(fit.grow.gls.1) ##

fit.grow.gls<-gls(size.t1~size.t+site.s,
                  na.action=na.omit,
                  weight=varExp(form=~fitted(.)), ## Exponential variance function structure of fitted data
                  method="ML",
                  data=growth_df); 
# summary(fit.grow.gls)
# plot(fit.grow.gls) ##

fit.grow.gls.0<-gls(size.t1~size.t+site.s,
                    na.action=na.omit,
                    ## no weight used
                    method="ML",
                    data=growth_df);
# summary(fit.grow.gls.0)
# plot(fit.grow.gls.0) ## 

## Compare models
anova(fit.grow.gls.0,fit.grow.gls,fit.grow.gls.1) 

# Remove models from environments
rm(fit.grow.gls,fit.grow.gls.0,fit.grow.gls.1)

######### check whether intercept estimate for habitat is needed #########
fit.grow.gls.0<-gls(size.t1~size.t, ## size only
                    na.action=na.omit,
                    weight=varExp(form=~fitted(.)|site.s),
                    method="ML",
                    data=growth_df);
# plot(fit.grow.gls.0)

fit.grow.gls.1<-gls(size.t1~size.t+site.s, ## size + site as independent terms
                    na.action=na.omit,
                    weight=varExp(form=~fitted(.)|site.s),
                    method="ML",
                    data=growth_df)
# plot(fit.grow.gls.1)

fit.grow.gls.2<-gls(size.t1~size.t*site.s, ## size*site as interaction term
                    na.action=na.omit,
                    weight=varExp(form=~fitted(.)|site.s),
                    method="ML",
                    data=growth_df)
# plot(fit.grow.gls.2)

## Compare models
anova(fit.grow.gls.0,fit.grow.gls.1,fit.grow.gls.2)

# Remove models from environment
rm(fit.grow.gls.0,fit.grow.gls.1,fit.grow.gls.2)

######### refit model with size and site main effects, and site specific decreasing variance #########
fit.grow.gls<-gls(size.t1~site.s+size.t-1, ## Why -1?
                  na.action=na.omit,
                  weight=varExp(form=~fitted(.)|site.s),
                  method="ML",
                  data=growth_df)

summary(fit.grow.gls)
intervals(fit.grow.gls) ## Confidence intervals on the parameters associated with the model

g.intercepts=fit.grow.gls$coef[1:2] ## Growth intercepts for each site
g.slopes=rep(fit.grow.gls$coef[3],2) ## Growth slopes for each site
var.exp.coef=fit.grow.gls$modelStruct$varStruct ## coef for variance structure
sigma.g=fit.grow.gls$sigma ## Residual standard error
```

#### Plot: Annual Growth
```{r plot_growth}

FUcol <- "black"
SPcol <- "blue"

ggplot() + 
  geom_abline(intercept = g.intercepts[1],slope = g.slopes[1], color=FUcol)+ # FU line
  geom_abline(intercept = g.intercepts[2],slope = g.slopes[2], linetype='longdash', color = SPcol)+ # SP line
  geom_abline(linetype='dashed') + # 45degree  line
  geom_point(data=growth_df,aes(x=size.t, y=size.t1, shape=site.s, color=site.s)) + # Size points for each site
  scale_shape_manual(name = "",values = c(16,1), labels = c("FU: n=735","SP: n=357")) +
  scale_color_manual(name = "",values = c(SPcol, FUcol), labels = c("FU: n=735","SP: n=357"))+
  theme_classic()+
  labs(x="Plant size year t", y="Plant size year t+1")+
  xlim(2,10)+
  ylim(2,10) +
  theme(legend.position = c(0.8, 0.2),
        legend.text = element_text(size=14))
```

#### Calculation: Flowering
```{r calc_flowering}

flower_df <- dataf_r1 %>% 
  dplyr::select(flow.s = flow.all,
                site.s = site.all,
                size.t = size.t.all) %>% 
  filter(complete.cases(.))

attach(flower_df)
# table(site.s)

store.size.flow=size.t[flow.s==1] # store size of plants that flowered
store.site.flow=site.s[flow.s==1] # store sites of plants that flowered

fit.flow.1=glm(flow.s~size.t*site.s, family=binomial) ## Fit flowering to binomial lm
fit.flow=glm(flow.s~size.t+site.s, family=binomial) 
fit.flow.0=glm(flow.s~size.t, family=binomial)

anova(fit.flow.0,fit.flow,fit.flow.1, test="Chisq") ## Why test with Chisq here specifically?

# Remove models from environments
rm(fit.flow,fit.flow.1,fit.flow.0)

fit.flow=glm(flow.s~site.s/size.t-1,family=binomial) ## Why '/'  and why -1?

f.intercepts=fit.flow$coef[1:2] ## Model intercepts
f.slopes=c(fit.flow$coef[3:4]) ## Model slopes

site.flow.SE=summary(fit.flow)$coef[5:6]
```

#### Plot: Flowering
```{r plot_flow}

flower_df$probability <- predict(fit.flow, site.s=flower_df$site.s, size.t = flower_df$size.t, type = "response")
predict_df <- data.frame(site.s=c(rep("FU",1000),rep("SP",1000)),
                         size.t=rep(seq(2,10,length.out = 1000),2))
predict_df <- cbind(predict_df,predict(fit.flow, newdata = predict_df, type = "response", se.fit = TRUE))


ggplot(data = predict_df, aes(x = size.t, y = fit)) +
  geom_line(aes(color = site.s, linetype=site.s)) +
  scale_color_manual(name = "", values=c(SPcol, FUcol),labels = c("FU: n=760","SP: n=398"))+
  scale_linetype_discrete(name = "",labels = c("FU: n=760","SP: n=398"))+
  scale_x_continuous(limits = c(2, 10)) +
  # geom_point(data=flower_df,aes(x=size.t,y=probability))+
  theme_classic()+
  labs(x="Plant size year t", y = "Probability of flowering (year t+1)")+
  theme(legend.position = c(0.9, 0.2),
        legend.text = element_text(size=14))

### Don't understand where their points around the lines come from. Is that related to binning?
detach(flower_df)
```

#### Calculation: Survival
```{r calc_survival}

survival_df <- dataf_r1 %>% 
  dplyr::select(surv.s = surv.all,
                site.s = site.all,
                size.t = size.t.all) %>% 
  filter(complete.cases(.))

attach(survival_df)
# table(site.s)

## 3 models to compare
fit.surv.1=glm(surv.s~size.t*site.s, family=binomial)
fit.surv=glm(surv.s~size.t+site.s, family=binomial)
fit.surv.0=glm(surv.s~size.t, family=binomial)

anova(fit.surv.0,fit.surv,fit.surv.1,test="Chisq")

rm(fit.surv.0,fit.surv,fit.surv.1)
fit.surv = glm(surv.s~site.s/size.t-1, family=binomial) ## Same as above -- why '/' and why -1?

s.intercepts = fit.surv$coef[1:2]
s.slopes = c(fit.surv$coef[3:4])

```

#### Plot: Survival
```{r plot_surv}

survival_df$probability <- predict(fit.surv, site.s=site.s, size.t = size.t, type = "response")

predict_df <- data.frame(site.s=c(rep("FU",1000),rep("SP",1000)),
                         size.t=rep(seq(2,10,length.out = 1000),2))
predict_df <- cbind(predict_df,predict(fit.surv, newdata = predict_df, type = "response", se.fit = TRUE))

ggplot(data = predict_df, aes(x = size.t, y = fit)) +
  geom_line(aes(color = site.s, linetype=site.s)) +
  scale_color_manual(name = "", values=c(SPcol, FUcol),labels = c("FU: n=809","SP: n=417"))+
  scale_linetype_discrete(name = "",labels = c("FU: n=809","SP: n=417"))+
  scale_x_continuous(limits = c(2, 10)) +
  scale_y_continuous(limits = c(0,1))+
  # geom_point(data=flower_df,aes(x=size.t,y=probability))+
  theme_classic()+
  labs(x="Plant size year t", y = "Probability of survival to year t+1")+
  theme(legend.position = c(0.9, 0.2),
        legend.text = element_text(size=14))

### Don't understand where their points around the lines come from. Is that related to binning?
detach(survival_df)
```

#### Calculation: Fecundity
```{r calc_fecundity}

IPM.fecundity=data.frame(read.table("CT.IPM.fecundity.txt", header=T))
# names(IPM.fecundity)

# Unbrowsed individuals
fecundity_df <- IPM.fecundity %>% 
  filter(brows.t1==0) %>% 
  dplyr::select(size.t.f=size.t,
                site.t.f=site,
                si.t1.f=si.t1,
                nl.t.f=nl.t)

attach(fecundity_df)

fit.fec = lm(log(si.t1.f)~ ## dep var = log of seeds per individual (unbrowsed) in year t+1
               site.t.f + ## site of unbrowsed individuals
               size.t.f-1) ## size of unbrowsed Why -1? #
```

#### Plot: Fecundity
```{r plot_fecundity, warning=FALSE}

ggplot() + 
  geom_abline(intercept = fit.fec$coefficients[1],slope = fit.fec$coefficients[3], color=FUcol)+ # FU line
  geom_abline(intercept = fit.fec$coefficients[2],slope = fit.fec$coefficients[3], linetype='longdash', color = SPcol)+ # SP line
  geom_point(data=fecundity_df,aes(x=size.t.f, y=log(si.t1.f), shape=site.t.f, color=site.t.f)) + # Size points for each site
  scale_shape_manual(name = "",values = c(16,1), labels = c("FU: n=735","SP: n=357")) +
  scale_color_manual(name = "",values = c(SPcol, FUcol), labels = c("FU: n=735","SP: n=357"))+
  theme_classic()+
  labs(x="Plant size year t", y="Plant size year t+1")+
  xlim(2,10)+
  ylim(2,10) +
  theme(legend.position = c(0.8, 0.2),
        legend.text = element_text(size=14))

detach(fecundity_df)
```

#### Seedling sizes
```{r sdlg_szs, message=FALSE}

####### Seedlings data #######
IPM.seedlings=data.frame(read.table("CT.IPM.seedlings.txt",header=T)) %>% 
  filter(site != "JU") %>% 
  dplyr::select(seedlings.size.t=size.t,
                seedlings.site=site)

fit.seedlings=lm(seedlings.size.t~seedlings.site-1, data = IPM.seedlings) ## That -1! What is that?
# summary(fit.seedlings)

## Add seedlings to all.sizes vector
size.all.plus.seedlings=c(all.sizes, IPM.seedlings$size.t[IPM.seedlings$year.t!=2003])
## Add site codes for seedlings to all sites vector
site.all.plus.seedlings=c(all.sizes, IPM.seedlings$site[IPM.seedlings$year.t!=2003])

####### Establishment data #######
IPM.establishment=data.frame(read.table("IPM.establishment.data.txt",header=T)) %>% 
  dplyr::select(p.est.site=site,
                p.est.seeds.t=seeds.t,
                p.est.seedlings=sdl.t1)

attach(IPM.establishment)

## Site specific seedling establishment rates
est.p.est = IPM.establishment %>% 
  group_by(p.est.site) %>% 
  summarise(est_rate=sum(p.est.seedlings)/sum(p.est.seeds.t))
est.p.est = as.numeric(est.p.est$est_rate)

detach(IPM.establishment)
```

#### Collect parameters
```{r collect_params}

## Set upper and lower plant size bounds
minsize<-1
maxsize<-12

# Global variables for midpoint rule approximation 
# n.big.matrix is the number of mesh points for size, n.age is the number of age classes. 
n.big.matrix = 250; n.age = 50; n=n.big.matrix ## 

L= minsize; U= maxsize;

# boundary points b and mesh points y
b = L+c(0:n)*(U-L)/n
y = 0.5*(b[1:n]+b[2:(n+1)])

# step size for midpoint rule, see equations 4 and 5
h = y[2]-y[1]

## initialize empty parameter matrix
store.p.vec = matrix(NA,12,2)
colnames(store.p.vec) <- c("FU","SP")
p.vec.names <- c("1st survival param","2nd survival param",
                  "1st flow param    ","2nd flow param    ",
                  "ag                ","bg                ",
                  "sigma2 growth     ","intercept seeds   ",
                  "slope seeds       ","mean kids size   ",
                  "sigma2 kids size ","growth variance parameter ")

## initialize empty parameter matrix
store.p.vec = matrix(NA,12,2)
colnames(store.p.vec) <- c("FU","SP")

## Store parameters in store.p.vec 12x2 array
for(i in 1:2){

	store.p.vec[1,i]<- as.numeric(s.intercepts[i])		# ; p.vec.names[1]<-"1st survival param";
	store.p.vec[2,i]<- s.slopes[i]				# ; p.vec.names[2]<-"2nd survival param";
	store.p.vec[3,i]<- f.intercepts[i]		# 	; p.vec.names[3]<-"1st flow param    ";
	store.p.vec[4,i]<- f.slopes[i]				# ; p.vec.names[4]<-"2nd flow param    ";
	store.p.vec[5,i]<- g.intercepts[i]		# 	; p.vec.names[5]<-"ag                ";
	store.p.vec[6,i]<- g.slopes[i]				# ; p.vec.names[6]<-"bg                ";
	store.p.vec[7,i]<- sigma.g^2				# ; p.vec.names[7]<-"sigma2 growth     ";
	store.p.vec[8,i]<- fit.fec$coef[i]   #			; p.vec.names[8]<-"intercept seeds   ";
	store.p.vec[9,i]<- fit.fec$coef[3]	 # 		; p.vec.names[9]<-"slope seeds       ";
	store.p.vec[10,i]<- fit.seedlings$coef[i]	# 	; p.vec.names[10]<-"mean kids size   ";
	store.p.vec[11,i]<- summary(fit.seedlings)$sigma^2	# ; p.vec.names[11]<-"sigma2 kids size ";
	store.p.vec[12,i]<- var.exp.coef[i]			# ; p.vec.names[12]<-"growth variance parameter ";

}
# summary(store.p.vec)
```

## Part II. Compute the kernel component functions from the fitted models

#### Basic demographic functions
```{r basic_functions}
## Survival model

sx <- function(x,params) {
	u <- exp(params[2]*x+params[1]); ## Logit model => odds
	return(u/(1+u)); ## odds to prob
}

## Fecundity model
fx<-function(x,params) {
	u<-exp(params[3]+params[4]*x) ## Logit model => odds
      return(u/(1+u))  ## odds to prob
}

## Growth model
gxy<-function(x,y,params) {
	mux<-params[5]+params[6]*x; ## growth slope and intercepts
	sigmax2<-(params[7])*exp(2*(params[12]*mux)) ## Variance around growth curve
	sigmax<-sqrt(sigmax2); ## Standard deviation around growth curve
	fac1<-sqrt(2*pi)*sigmax; ## ??
	fac2<-((y-mux)^2)/(2*sigmax2); ## ??
	return(exp(-fac2)/fac1); ## ??
}

## Survival-growth function
pxy<-function(x,y,params) {
  return(sx(x,params)*(1-fx(x,params))*gxy(x,y,params))}

## Fecundity function
fxy<-function(x,y,params) {
	nkids<-p.est*exp(params[8]+params[9]*x) ## 8: seeds intercept; 9: seeds slope
	kidsize.mean<- params[10] ## Mean seedling size
	kidsize.var<- params[11] ## Sigma seedling size
	fac1<-sqrt(2*pi)*sqrt(kidsize.var) ## Same Q as 483 above
	fac2<-((y-kidsize.mean)^2)/(2*kidsize.var) ## 484 above
	f<-sx(x,params)*fx(x,params)*nkids*exp(-fac2)/fac1 ## End is 485 above
	return(f);
}
```

#### The 'big matrix' M of size n x n
```{r big_matrix}

bigmatrix<-function(n,params) {

  # upper and lower integration limits
	L<-minsize; U<-maxsize;
	
  # boundary points b and mesh points y
	b<-L+c(0:n)*(U-L)/n; # Boundaries
	y<-0.5*(b[1:n]+b[2:(n+1)]); # Midpoints

  # construct the matrix
  I <- diag(n); ## Identity matrix
  
  ## Survival-growth matrix for all midpoint sizes
	P<-t(outer(y,y,pxy,params=params)) # P: survival-growth transpose outer product. Why transpose?
	
	## fecundity matrix for all midpoint sizes
	B<-t(outer(y,y,fxy,params=params)) # B: fecundity
	
	## Empty matrix of zeroes
	M=array(0,dim=c(2*n,2*n))
	
	## Insert P matrix
	M[1:n,1:n]=P*(U-L)/n
	
	## Insert B matrix to the right of the P matrix
	M[1:n,(n+1):(2*n)]=B*(U-L)/n
	
	## Insert the identity matrix
	M[(n+1):(2*n),1:n]=diag(n)
	K<-M ## Call K to match paper?
  P<-(U-L)*P/n
  B<-(U-L)*B/n
	return(list(matrix=M,
	            kernel=K,
	            meshpts=y,
	            Pmatrix=P,
	            Bmatrix=B,
	            Imatrix=I)); 
}


R0.calc<-function(n,params){
	
  M<-bigmatrix(n,params) ## Construct matrix
	
  ## If any NAs in matrix, return NA for estimates
	if (any(is.na(M$matrix))){
		ave.R0=NA
		lam=NA
		T=NA } 
  
  else{
		N <- solve(M$Imatrix-M$Pmatrix) ## ?? Some linear algebra operation
		R <- M$Bmatrix %*% N ## Matrix multiplication
		ave.R0<-Re(eigen(R)$values[1]) ## Re: real; eigen:returns vector of eigenvalues. values[1] is the first eigenvalue in returned vector 
		lam<-Re(eigen(M$matrix)$values[1]); ## Re: real; eigen:returns vector of eigenvalues. values[1] is the first eigenvalue in returned vector 
		T=log(ave.R0)/log(lam) ## ?? understand why this is true
	}

	return(list(lam=lam,ave.R0=ave.R0,T=T))
}

R0.betas<-function(x){
  
	p.vec[3] <- x; ## p.vec[3] = 1st flow pram. Assign value of x to that
	nR0 <- R0.calc(n.big.matrix, p.vec) 
	return(nR0$ave.R0)
	
}

```

#### Calculation: generation time
```{r calc_T}

gen.time=rep(NA,2)

for(i in 1:2){
	#if(i==1) p.est= 8.604605e-05 else p.est=0.0001655622  # assuming dd-reg
	if(i==1) p.est= est.p.est[1] else p.est=est.p.est[2]   # actual ## site specific parameters
	p.vec=store.p.vec[,i] ## Site-specific parameters
	tmp=R0.calc(n.big.matrix,p.vec) 
	gen.time[i]=tmp$T ## get T from list of value returned from R0.calc
	cat("Site ",i," lambda=",tmp$lam," R0=",tmp$ave.R0," Generation time=",tmp$T,"\n") ## Print results
	cat("ESS intercept ", optimize(R0.betas, c(-100,10), maximum=T, tol=0.01)$maximum,"\n") ## Print results
}

```

#### Calculation: Evolutionarily stable strategy
```{r calc_ESS}

n.test <- 100 ##
R0.beta <- array(NA,dim=c(n.test,2)); ## 100x2 matrix array of NAs
lam.beta <- array(NA,dim=c(n.test,2)); ## 100x2 matrix array of NAs
ESS = rep(NA,2) ## 2x1 vector of NAs to store ESS

```

#### Plot: Evolutionarily stable strategy
```{r plot_ESS}

dev.new() ## Start new plot window
par(mfrow=c(2,2), mar=c(3,3,1,2)+0.1, bty="l",pty="s", cex.main=1, cex.axis=1, cex.lab=1, tck=0.02, mgp=c(2, 0.3 ,0)) ;

## 
for(i in 1:2){
	p.vec=store.p.vec[,i] ## Site specific parameters
	#if(i==1) p.est= 8.604605e-05 else p.est=0.0001655622  # assuming dd-reg
	if(i==1) p.est= est.p.est[1] else p.est=est.p.est[2]   # actual
	if(i==1) beta.flow<-seq(-100,0,length=n.test) else beta.flow<-seq(-50,0,length=n.test); ## Why -50 for SP and -100 for FU?

	## Beta test -- 
	for(beta.test in 1:n.test){
		
	  p.vec[3]<-beta.flow[beta.test]; ## Assign flowering intercept
		nR0<-R0.calc(n.big.matrix,p.vec) ## Get R0 associated with the flowering intercept above
		R0.beta[beta.test,i]<-nR0$ave.R0 ## Fill ave.R0 into R0beta matrix that was created above
		lam.beta[beta.test,i]<-nR0$lam ## Same for lambda
		cat(beta.flow[beta.test],"  ",nR0$ave.R0,"  ",nR0$lam,"\n")
		
	}

	## Put ESS into matrix created above. ESS here is flowering intercept where R0 is highest
	ESS[i]<-beta.flow[R0.beta[,i]==max(R0.beta[,i])]

	## plot
	plot(beta.flow,R0.beta[,i],
	     type="n",
	     xlab=expression("Intercept of flowering function " * italic(beta * scriptstyle(0))),
	     ylab=expression(italic("R"*scriptstyle(0))))
	
	min.R0=min(R0.beta[,i]); max.R0=max(R0.beta[,i])
	mean.m2se=fit.flow$coef[i]-2*site.flow.SE[i] ## lower boundary for confidence interval
  mean.p2se=fit.flow$coef[i]+2*site.flow.SE[i] ## upper boundary for confidence interval

	polygon(c(mean.m2se,mean.p2se,mean.p2se,mean.m2se),c(min.R0,min.R0,max.R0,max.R0), col="grey90",border=0)
	points(beta.flow,R0.beta[,i],type="l")
	abline(h=1)
	points(beta.flow[R0.beta[,i]==max(R0.beta[,i])],max(R0.beta[,i]),pch=19)
	abline(v=fit.flow$coef[i])
	#abline(v=beta.flow[R0.beta==max(R0.beta)])

	# if (i==1) text(2.3,10,"a)") else text(locator(1),"c)")

	plot(beta.flow,lam.beta[,i],type="n",xlab=expression("Intercept of flowering function " * italic(beta * scriptstyle(0))),
	ylab=expression(italic(lambda)))
	min.R0=min(lam.beta[,i]);max.R0=max(lam.beta[,i])
	mean.m2se=fit.flow$coef[i]-2*site.flow.SE[i]
	mean.p2se=fit.flow$coef[i]+2*site.flow.SE[i]

	polygon(c(mean.m2se,mean.p2se,mean.p2se,mean.m2se),c(min.R0,min.R0,max.R0,max.R0), col="grey90",border=0)
	points(beta.flow,lam.beta[,i],type="l")
	abline(h=1)
	abline(v=fit.flow$coef[i])
	points(beta.flow[lam.beta[,i]==max(lam.beta[,i])],max(lam.beta[,i]),pch=19)

	# if (i==1) text(locator(1),"b)") else text(locator(1),"d)")
}
```


####	Constructing the component matrices and their transposes 
```{r component_matrices}
# Put all component matrices into 3-dimensional arrays 
P <- array(NA,dim=c(n.big.matrix,n.big.matrix)) #P[j,i,a] will be h*P_{a-1}(x_j,x_i)
B <- array(NA,dim=c(n.big.matrix,n.big.matrix)) #B[j,i,a] will be h*F_{a-1}(x_j,x_i)

stable.dist=array(NA,dim=c(n,n.age,2))
lam.stable.age=rep(NA,2);

for(i in 1:2){
	p.vec=store.p.vec[,i]
	if(i==1) p.est=est.p.est[1] else p.est=est.p.est[2]
	P<-h*t(outer(y,y,pxy,params=p.vec))
	B<-h*t(outer(y,y,fxy,params=p.vec))

#============================================================================================#
#	Model iteration functions  
#============================================================================================#

	# population now and next year 
	Nt=matrix(0,n.big.matrix,n.age); 
	Nt1=Nt 
	Nt2=Nt	
	
	iteration=function(Nt1,Nt){
		for(age in 2:n.age){
			Nt2[,age]=P%*%Nt1[,age-1]
			}
		Nt2[,1]=0; 
		
		for(age in 1:n.age){
			Nt2[,1]=Nt2[,1]+B%*%Nt[,age]
			}
		return(Nt2)
		}

  #============================================================================================#
  #	Start using the model 
  #============================================================================================#

  # Estimate lambda and w by iterating unperturbed matrix  
	Nt1=matrix(1,n.big.matrix,n.age); 
	Nt=Nt1
	qmax=1000; 
	lam=1; 
	tol=1.e-8; 
	while(qmax>tol) {
		Nt2=iteration(Nt1,Nt);
		qmax=sum(abs(Nt2-lam*Nt1));  
		lam=sum(Nt2)/sum(Nt1); 
	
		Nt=Nt1
		Nt1=Nt2
	
		tot=sum(Nt1+Nt2)
	
		Nt=Nt/tot
		Nt1=Nt1/tot
	 
		cat(lam,qmax,"\n");
	} 

	stable.dist[,,i]=Nt/sum(Nt); lam.stable.age[i]=lam; 	

}
```

#### Calculation: Stable distribution and size-dependent total elasticity 
```{r calc_stable_distribution}
stable.dist.flow=stable.dist

p.surv.flow=sx(y,p.vec)*fx(y,p.vec)

for(i in 1:2){
	for(age in 1:n.age){
		stable.dist.flow[,age,i]=stable.dist[,age,i]*p.surv.flow
	}
}

for(i in 1:2){
	stable.dist.flow[,,i]=stable.dist.flow[,,i]/(sum(stable.dist.flow[,,i]))
}

dataf=data.frame(read.table("ipm.saf.data.txt",header=T))
attach(dataf)

flow.age=age.05[flo.05==1]
flow.site=as.numeric(site[flo.05==1])
age=age.05
site=as.numeric(site)

stable.dist.age=array(NA,dim=c(n.age,2))
stable.dist.age.flow=array(NA,dim=c(n.age,2))
```

##### Plot: Stable age distribution (see Fig. 2)
```{r plot_stable_distribution}
dev.new()
par(mfrow=c(2,2), mar=c(3,3,1,2)+0.1, bty="l",pty="s", cex.main=1, cex.axis=1, cex.lab=1, tck=0.02, mgp=c(2, 0.3 ,0)) ;

for(i in 1:2){
	hist(age[site==i],breaks=seq(0,50,3),freq=F,col="grey",main="",xlab="Age (years)", ylim=c(0,0.2))
	stable.dist.age[,i]=apply(stable.dist[,,i],2,sum)
	points(0:(n.age-1),stable.dist.age[,i],type="l")
	# if (i==1) text(locator(1),"a)") else text(locator(1),"c)")

	hist(flow.age[flow.site==i],breaks=seq(0,50,3),freq=F,col="grey",main="",xlab="Age (years)",ylim=c(0,0.20))
	stable.dist.age.flow[,i]=apply(stable.dist.flow[,,i],2,sum)
	points(0:(n.age-1),stable.dist.age.flow[,i],type="l")
	# if (i==1) text(locator(1),"b)") else text(locator(1),"d)")
}

mean.age.f=rep(NA,2)
mean.size.f=rep(NA,2)

for(i in 1:2){
	mean.age.f[i] <-sum((1:(n.age))*apply(stable.dist.flow[,,i],2,sum))-1;
	cat("Mean flowering age",mean.age.f[i],"\n")
	mean.size.f[i]<-sum(exp(y)*apply(stable.dist.flow[,,i],1,sum));
	cat("Mean flowering size",mean.size.f[i],"\n")
}

stable.dist.size=array(NA,dim=c(n.big.matrix,2))
stable.dist.size.flow=array(NA,dim=c(n.big.matrix,2))
```

#### Stable size distribution (see Fig. 3)
```{r plot_stable_size}

dev.new()
par(mfrow=c(2,2), mar=c(3,3,1,2)+0.1, bty="l",pty="s", cex.main=1, cex.axis=1, cex.lab=1, tck=0.02, mgp=c(2, 0.3 ,0)) ;

for(i in 1:2){
	hist(size.all.plus.seedlings[site.all.plus.seedlings==i],freq=T,col="grey",main="",xlab="Plant size",breaks=seq(1,10.5,0.5))
	stable.dist.size[,i]=sum(!is.na(size.all.plus.seedlings[site.all.plus.seedlings==i]))*apply(stable.dist[,,i],1,sum)/(y[2]-y[1])/2
	points(y,stable.dist.size[,i],type="l")
	# if (i==1) text(locator(1),"a)") else text(locator(1),"c)")

	hist(store.size.flow[store.site.flow==site.code.l[i]],freq=T,col="grey",main="",xlab="Plant size",breaks=seq(2,9.5,0.5))
	stable.dist.size.flow[,i]=sum(!is.na(store.size.flow[store.site.flow==site.code.l[i]]))*apply(stable.dist.flow[,,i],1,sum)/(y[2]-y[1])/2
	points(y,stable.dist.size.flow[,i],type="l")
	# if (i==1) text(locator(1),"b)") else text(locator(1),"d)")
}
```

#### Iterate model with time lag
```{r time_lag_iteration}

stable.dist.tl=array(0,dim=c(n,2))
lam.stable.tl=rep(NA,2)
b = L+c(0:n)*(U-L)/n; y = 0.5*(b[1:n]+b[2:(n+1)]);
h = y[2]-y[1]

for(i in 1:2){
	p.vec=store.p.vec[,i]
	if(i==1) p.est= est.p.est[1] else p.est=est.p.est[2]               
	P<-h*t(outer(y,y,pxy,params=p.vec))
	B<-h*t(outer(y,y,fxy,params=p.vec))

	qmax=1000

	# population now, next year and the one after
	Nt=matrix(1/n,n); 
	Nt1=Nt/2;  

	while(qmax>1e-10) {
		Nt2=P%*%Nt1+B%*%Nt
		qmax=sum(abs(Nt2-lam*Nt1));  
		lam=sum(Nt2)/sum(Nt1); 
		Nt=Nt1;
		Nt1=Nt2;
		tot=sum(Nt+Nt1)
		Nt=Nt/tot
		Nt1=Nt1/tot
	} 
	stable.dist.tl[,i]=Nt/sum(Nt); lam.stable.tl[i]=lam; 
}

lam.stable.age
lam.stable.tl
```

#### Calculation: sensitivity and elasticity by perturbation P matrix
```{r calc_Sens_Elas}

sen.big.P<-array(NA,dim=c(n,n)) 	#array to store the results 
elas.big.P<-array(NA,dim=c(n,n,2)) 

for(i in 1:2){
	p.vec=store.p.vec[,i]
	if(i==1) p.est= est.p.est[1] else p.est=est.p.est[2]               #actual

	P<-h*t(outer(y,y,pxy,params=p.vec))
	B<-h*t(outer(y,y,fxy,params=p.vec))


	for(row in 1:n) {		# loop over y values 
		# choose x* to maximize e(y,x) for this y value, by scanning across the row    
		big.one=which(P[row,]*stable.dist.tl[,i]==max(P[row,]*stable.dist.tl[,i])); 

		# perturb the kernel up and down near (y,x*)
		delta=0.1*h*P[row,big.one];
		Pup=P; Pup[row,big.one] = P[row,big.one]+delta/h;
		Pdown=P; Pdown[row,big.one] = P[row,big.one]-delta/h;  
	
		qmax=1; lamup=1; lamdown=1; 
		Nt.up<-stable.dist.tl[,i]; Nt1.up<-stable.dist.tl[,i]
		Nt.down<-stable.dist.tl[,i]; Nt1.down<-stable.dist.tl[,i]
	
		while(qmax>1e-10) {
			Nt2.up=Pup%*%Nt1.up+B%*%Nt.up
			qmax=sum(abs(Nt2.up-lamup*Nt1.up));  
			lamup=sum(Nt2.up)/sum(Nt1.up); 

			Nt.up=Nt1.up;
			Nt1.up=Nt2.up;
			tot=sum(Nt.up+Nt1.up)
			Nt.up=Nt.up/tot
			Nt1.up=Nt1.up/tot

			Nt2.down=Pdown%*%Nt1.down+B%*%Nt.down

			qmax=qmax+sum(abs(Nt2.down-lamdown*Nt1.down));  
			lamdown=sum(Nt2.down)/sum(Nt1.down); 

			Nt.down=Nt1.down;
			Nt1.down=Nt2.down;
			tot=sum(Nt.down+Nt1.down)
			Nt.down=Nt.down/tot
			Nt1.down=Nt1.down/tot

			#cat(lamup,lamdown,qmax,"\n");
		} 


		sen.big.row<-(lamup-lamdown)/(2*delta) #sensitivity for perturbation at (y,x*)
		sen.big.P[row,]<-(stable.dist.tl[,i]/stable.dist.tl[,i][big.one])*sen.big.row #sensitivity at other x's 
		cat(row,big.one,lamup,lamdown," sens=",sen.big.row, "\n")
	}

	elas.big.P[,,i]=(P/h)*sen.big.P/lam.stable.tl[i];

}

sum(elas.big.P[,,1]*h*h)
sum(elas.big.P[,,2]*h*h)
```

#### Calculation: sensitivity and elasticity by perturbation B matrix
```{r perturb_ElasSens}

sen.big.B<-array(NA,dim=c(n,n)) 	#array to store the results 
elas.big.B<-array(NA,dim=c(n,n,2)) 

for(i in 1:2){
	p.vec=store.p.vec[,i]
	if(i==1) p.est= est.p.est[1] else p.est=est.p.est[2]               #actual

	P<-h*t(outer(y,y,pxy,params=p.vec))
	B<-h*t(outer(y,y,fxy,params=p.vec))

	for(row in 1:n) {		# loop over y values 
		# choose x* to maximize e(y,x) for this y value, by scanning across the row    
		big.one=which(B[row,]*stable.dist.tl[,i]==max(B[row,]*stable.dist.tl[,i])); 

		# perturb the kernel up and down near (y,x*)
		delta=0.1*h*B[row,big.one];
		Bup=B; Bup[row,big.one] = B[row,big.one]+delta/h;
		Bdown=B; Bdown[row,big.one] = B[row,big.one]-delta/h;  
	
		qmax=1; 
		lamup=1; 
		lamdown=1;
		
		Nt.up<-stable.dist.tl[,i]; 
		Nt1.up<-stable.dist.tl[,i]
		
		Nt.down<-stable.dist.tl[,i]; 
		Nt1.down<-stable.dist.tl[,i]
	
		while(qmax>1e-10) {
			Nt2.up=P%*%Nt1.up+Bup%*%Nt.up
			qmax=sum(abs(Nt2.up-lamup*Nt1.up));  
			lamup=sum(Nt2.up)/sum(Nt1.up); 

			Nt.up=Nt1.up;
			Nt1.up=Nt2.up;
			tot=sum(Nt.up+Nt1.up)
			Nt.up=Nt.up/tot
			Nt1.up=Nt1.up/tot

			Nt2.down=P%*%Nt1.down+Bdown%*%Nt.down

			qmax=qmax+sum(abs(Nt2.down-lamdown*Nt1.down));  
			lamdown=sum(Nt2.down)/sum(Nt1.down); 

			Nt.down=Nt1.down;
			Nt1.down=Nt2.down;
			tot=sum(Nt.down+Nt1.down)
			Nt.down=Nt.down/tot
			Nt1.down=Nt1.down/tot

			#cat(lamup,lamdown,qmax,"\n");
		} 

		sen.big.row<-(lamup-lamdown)/(2*delta) #sensitivity for perturbation at (y,x*)
		sen.big.B[row,]<-(stable.dist.tl[,i]/stable.dist.tl[,i][big.one])*sen.big.row #sensitivity at other x's 
		cat(row,big.one," sens=",sen.big.row, "\n")
	}

	elas.big.B[,,i]=2*(B/h)*sen.big.B/lam.stable.tl[i];

}

sum(elas.big.B[,,1]*h*h)+sum(elas.big.P[,,1]*h*h)
sum(elas.big.B[,,2]*h*h)+sum(elas.big.P[,,2]*h*h)
```

#### Plot: elasticity (see Fig 5)
```{r plot_elasticity}

dev.new()
par(mfrow=c(2,2), mar=c(3,3,1,2)+0.1, bty="l",pty="s", cex.main=1, cex.axis=1, cex.lab=1, tck=0.02, mgp=c(2, 0.3 ,0)) ;

zmax=max(elas.big.P,elas.big.B)

image(y,y,t(elas.big.P[,,1]),xlab="Plant size year t",ylab="Plant size year t+1",col=grey(1:300/300),gamma=0.2,zlim=c(0,1.1*zmax));
	contour(y,y,t(elas.big.P[,,1]),add=T,cex=3,levels = c(0.01,0.05,0.1,0.15,0.2,0.25,0.3));
	# text(locator(1),"a)",col="white")

image(y,y,t(elas.big.B[,,1]),xlab="Plant size year t-1",ylab="Plant size year t+1",col=grey(1:300/300),gamma=0.2,zlim=c(0,1.1*zmax));
	contour(y,y,t(elas.big.B[,,1]),add=T,cex=3,levels = c(0.01,0.05,0.1,0.15,0.2,0.25,0.3));
	# text(locator(1),"b)",col="white")

image(y,y,t(elas.big.P[,,2]),xlab="Plant size year t",ylab="Plant size year t+1",col=grey(1:300/300),gamma=0.2,zlim=c(0,1.1*zmax));
	contour(y,y,t(elas.big.P[,,2]),add=T,cex=3,levels = c(0.01,0.05,0.1,0.15,0.2,0.25,0.3));
	# text(locator(1),"c)",col="white")

image(y,y,t(elas.big.B[,,2]),xlab="Plant size year t-1",ylab="Plant size year t+1",col=grey(1:300/300),gamma=0.2,zlim=c(0,1.1*zmax));
	contour(y,y,t(elas.big.B[,,2]),add=T,cex=3,levels = c(0.01,0.05,0.1,0.15,0.2,0.25,0.3));
	# text(locator(1),"d)",col="white")
```

#### Calculation: new big matrix approximation
```{r calc_new_bigM}

M.tl=array(0,dim=c(2*n,2*n))
lam.stable.bm=rep(NA,2)
R0.stable.bm=rep(NA,2)

for(i in 1:2){
	p.vec=store.p.vec[,i]
	if(i==1) p.est= est.p.est[1] else p.est=est.p.est[2]              
	if(i==1) p.vec[3]=-58.67228 else p.vec[3]= -26.25266              

	P<-h*t(outer(y,y,pxy,params=p.vec))
	B<-h*t(outer(y,y,fxy,params=p.vec))

	M.tl[1:n,1:n]=P
	M.tl[1:n,(n+1):(2*n)]=B
	M.tl[(n+1):(2*n),1:n]=diag(n)

	lam.stable.bm[i]=Re(eigen(M.tl)$values[1]);

	#R0
	M.P=array(0,dim=c(2*n,2*n))
	M.B=array(0,dim=c(2*n,2*n))

	M.P[1:n,1:n]=P
	M.P[(n+1):(2*n),1:n]=diag(n)

	M.B[1:n,(n+1):(2*n)]=B


	N<-solve(diag(2*n)-M.P);
	R<- M.B %*% N
	R0.stable.bm[i]<-Re(eigen(R)$values[1]);

	# Generation time
	T.stable.mb <-log(R0.stable.bm)/log(lam.stable.bm)	
}


lam.stable.bm	# Lambda
R0.stable.bm	# Net reproductive rate
T.stable.mb 	# Generation time
```

#### Simulate Lambda, R0 and T with varying p.est
```{r simulate_R0_T_Lambda}

seq.start <- 0.00001
seq.end <- 0.001
seq.by <- 0.00001
p.est.seq <- seq(seq.start,seq.end,by=seq.by)
n.p.est <- length(p.est.seq)

p.est.FU <- 0.00016
p.est.SP <- 0.00078

Res.p.est <- matrix(0,n.p.est,8)
dimnames(Res.p.est) <- list(1:n.p.est,c("p.est","p.est","l.FU","l.SP","Ro.FU","Ro.SP","T.FU","T.SP"))

#new big matrix approximation
M.tl=array(0,dim=c(2*n,2*n))
lam.stable.bm=rep(NA,2)
R0.stable.bm=rep(NA,2)

for (k in 1:n.p.est){
	for(i in 1:2){
		p.est= p.est.seq[k]
		p.vec=store.p.vec[,i]
		P<-h*t(outer(y,y,pxy,params=p.vec))
		B<-h*t(outer(y,y,fxy,params=p.vec))
		M.tl[1:n,1:n]=P
		M.tl[1:n,(n+1):(2*n)]=B
		M.tl[(n+1):(2*n),1:n]=diag(n)
		lam.stable.bm=Re(eigen(M.tl)$values[1]);

	# R0
		M.P=array(0,dim=c(2*n,2*n))
		M.B=array(0,dim=c(2*n,2*n))
		M.P[1:n,1:n]=P
		M.P[(n+1):(2*n),1:n]=diag(n)
		M.B[1:n,(n+1):(2*n)]=B
		N<-solve(diag(2*n)-M.P);
		R<- M.B %*% N
		R0.stable.bm<-Re(eigen(R)$values[1]);
	
	# Generation time
		T.stable.bm <-log(R0.stable.bm)/log(lam.stable.bm)	

	# Filling in of result matrix 
		Res.p.est[k,i] <- p.est
		Res.p.est[k,i+2] <- lam.stable.bm
		Res.p.est[k,i+4] <- R0.stable.bm
		Res.p.est[k,i+6] <- T.stable.bm
	}
}

Res.p.est
```

#### Plot: Simulated Lambda, R0 and T with varying p.est (see Figure 4)
```{r plot_sim}

# dev.new()
par(mfrow=c(1,3), mar=c(3,3,1,2)+0.1, bty="l",pty="s", cex.main=1, cex.axis=1, cex.lab=1, tck=0.02, mgp=c(2, 0.3 ,0)) ;

plot(Res.p.est[,1],Res.p.est[,3],type="n",xlab=expression(paste(italic(p)[e])),ylab=expression(lambda))
	lines(Res.p.est[,1],Res.p.est[,3],lty=1)
	lines(Res.p.est[,1],Res.p.est[,4],lty=2)
	points(p.est.FU,1.05, pch=19, cex=2.0); points(p.est.SP,1.17, pch=1, cex=2.0); 
	legend(0.0006,1,c("FU", "SP"),pch=c(19,1),lty=c(1,2),bty="n",xjust=0,cex=1.5)
	
plot(Res.p.est[,1],Res.p.est[,5],type="n",xlab=expression(paste(italic(p)[e])),ylab=expression(paste(italic(R[0]))))
	lines(Res.p.est[,1],Res.p.est[,5],lty=1)
	lines(Res.p.est[,1],Res.p.est[,6],lty=2)
	points(p.est.FU,1.67, pch=19, cex=2.0); points(p.est.SP,4.97, pch=1, cex=2.0)

plot(Res.p.est[,1],Res.p.est[,7],type="n",xlab=expression(paste(italic(p)[e])),ylab=expression(italic(T)),ylim=c(10,13))
	lines(Res.p.est[,1],Res.p.est[,7],lty=1)
	lines(Res.p.est[,1],Res.p.est[,8],lty=2)
	points(p.est.FU,10.82, pch=19, cex=2.0); points(p.est.SP,10.37, pch=1, cex=2.0)

```

```{r bootstrap}
## Comment out bootstrap because takes multiple days to run -- way to convert to parallel processing?
	
# #============================================================================# 
# #  Bootstrap lambda, R0 and generation time (see Appendix S4)
# #============================================================================# 
# 
# 
# n.boot=5000
# dem.stats=array(NA,dim=c(n.boot,3,2))
# boot.ESS=array(NA,dim=c(n.boot,2))
# 
# M.tl=array(0,dim=c(2*n,2*n))
# 
# for(b.samp in 1:n.boot){
# 
# 	#growth
# 	
# 	size.t=size.t.all[flow.all==0]
# 	size.t1=size.t1.all[flow.all==0]
# 	site.s=site.all[flow.all==0]
# 	test=complete.cases(size.t,size.t1,site.s)
# 	size.t=size.t[test]
# 	size.t1=size.t1[test]
# 	site.s=site.s[test]
# 	sample.boot=c(sample(1:735,replace=T),735+sample(1:357,replace=T))
# 	size.t.boot=size.t[sample.boot]
# 	size.t1.boot=size.t1[sample.boot]
# 	fit.grow.gls.boot<-gls(size.t1.boot~site.s+size.t.boot-1,na.action=na.omit,weight=varExp(form=~fitted(.)|site.s),method="ML");
# 	g.intercepts.boot=fit.grow.gls.boot$coef[1:2]
# 	g.slopes.boot=rep(fit.grow.gls.boot$coef[3],2)
#  	var.exp.coef.boot=fit.grow.gls.boot$modelStruct$varStruct  
# 	sigma.g.boot=fit.grow.gls.boot$sigma
# 
# 	#survival and flowering
# 
# 	sample.boot=c(sample(1:827,replace=T),827+sample(1:746,replace=T))
# 	size.t.boot=size.t.all[sample.boot]
# 	flow.all.boot=flow.all[sample.boot]
# 	surv.all.boot=surv.all[sample.boot]
# 	fit.flow.boot=glm(flow.all.boot~site.all*size.t.boot-1,family=binomial)
# 	f.intercepts.boot=fit.flow.boot$coef[1:2]
# 	f.slopes.boot=c(fit.flow.boot$coef[3],fit.flow.boot$coef[3]+fit.flow.boot$coef[4])
# 	fit.surv.boot=glm(surv.all.boot~site.all*size.t.boot-1,family=binomial)
# 	s.intercepts.boot=fit.surv.boot$coef[1:2]
# 	s.slopes.boot=c(fit.surv.boot$coef[3],fit.surv.boot$coef[3]+fit.surv.boot$coef[4])
# 
# 	#fecundity
# 
# 	sample.boot=c(sample(1:25,replace=T),25+sample(1:28,replace=T))
#       size.t.f.boot=size.t.f[sample.boot]
#       si.t1.f.boot=si.t1.f[sample.boot]
#       fit.fec.boot=lm(log(si.t1.f.boot)~site.t.f+size.t.f.boot-1)
# 
# 	#seedlings
# 
# 	sample.boot=c(sample(1:573,replace=T),573+sample(1:123,replace=T))
# 	seedlings.size.t.boot=seedlings.size.t[sample.boot]
# 	fit.seedlings.boot=lm(seedlings.size.t.boot~seedlings.site-1)
# 
# 	#p.est
# 
# 	sample.boot=c(sample(1:70,replace=T),70+sample(1:42,replace=T))	
# 	p.est.seeds.t.boot=p.est.seeds.t[sample.boot]
# 	p.est.seedlings.boot=p.est.seedlings[sample.boot]
# 	est.p.est.boot= sapply(split(p.est.seedlings.boot,p.est.site),sum)/sapply(split(p.est.seeds.t.boot,p.est.site),sum)
# 
# 	
# 	for(i in 1:2){
# 		p.vec[1]<- s.intercepts.boot[i]				
# 		p.vec[2]<- s.slopes.boot[i]				
# 		p.vec[3]<- f.intercepts.boot[i]				
# 		p.vec[4]<- f.slopes.boot[i]				
# 		p.vec[5]<- g.intercepts.boot[i]				
# 		p.vec[6]<- g.slopes.boot[i]				
# 		p.vec[7]<- sigma.g.boot^2				
# 		p.vec[8]<- fit.fec.boot$coef[i]  			
# 		p.vec[9]<- fit.fec.boot$coef[3]		 		
# 		p.vec[10]<- fit.seedlings.boot$coef[i]			
# 		p.vec[11]<- summary(fit.seedlings.boot)$sigma^2		
# 		p.vec[12]<- var.exp.coef.boot[i]	
# 
# 		if(i==1) p.est= est.p.est.boot[1] else p.est=est.p.est.boot[2] 
# 	
# 		P<-h*t(outer(y,y,pxy,params=p.vec))
# 		B<-h*t(outer(y,y,fxy,params=p.vec))
# 
# 		M.tl=array(0,dim=c(2*n,2*n))
# 		M.tl[1:n,1:n]=P
# 		M.tl[1:n,(n+1):(2*n)]=B
# 		M.tl[(n+1):(2*n),1:n]=diag(n)
# 
# 		lam=Re(eigen(M.tl)$values[1]);
# 
# 	#R0
# 		
# 		M.P=array(0,dim=c(2*n,2*n))
# 		M.B=array(0,dim=c(2*n,2*n))
# 
# 		M.P[1:n,1:n]=P
# 		M.P[(n+1):(2*n),1:n]=diag(n)
# 
# 		M.B[1:n,(n+1):(2*n)]=B
# 
# 		N<-solve(diag(2*n)-M.P);
# 		R<- M.B %*% N
# 		R0<-Re(eigen(R)$values[1]);
# 
# 	#generation time
# 	
# 		T=log(R0)/log(lam)
# 		boot.data=c(lam,R0,T)
# 	
# 		dem.stats[b.samp,,i]=boot.data
# 
# 		if(any(!is.na(boot.data))){ 
# 			boot.ESS[b.samp,i]<-optimize(R0.betas,c(-150,10),maximum=T,tol=0.01)$maximum
# 		} else boot.ESS[b.samp,i]=NA
# 	}
# 	cat("sample ",b.samp,"\n")
# }	
# 
# getStats=function(x){
# 	ci.normal.app=c(mean(x)-1.96*sd(x),mean(x)+1.96*sd(x))
# 	res=c(mean(x,na.rm=T),quantile(x,p=c(0.025,0.5,0.975),na.rm=T),ci.normal.app)
# 	return(res)
# 	}
# 
# for(i in 1:2){
# 	cat("site ",site.code.l[i],"\n")
# 	print(apply(dem.stats[,,i],2,getStats))
# 	print(getStats(boot.ESS[,i]))
# }
# 
# 
# #============================================================================# 
# #  Plot: Bootstrap lambda, R0 and generation time (see Appendix S4)
# #============================================================================# 
# 
# 
# dev.new()
# par(mfrow=c(2,4), mar=c(3,3,1,2)+0.1, bty="l",pty="s", cex.main=1, cex.axis=1, cex.lab=1, tck=0.02, mgp=c(2, 0.3 ,0)) ;
# 
# main.titles=c(expression(italic(lambda)),expression(italic(R[0])),"Generation time, T",expression("ESS " * italic(beta * scriptstyle(0))))
# 
# for(i in 1:2){
# 	for(j in 1:3){
# 		hist(dem.stats[,j,i][dem.stats[,j,i]<30],xlab=main.titles[j],col="grey",main="")
# 		abline(v=mean(dem.stats[,j,i],na.rm=T))
# 		abline(v=median(dem.stats[,j,i],na.rm=T),col="blue")
# 		abline(v=quantile(dem.stats[,j,i],p=0.025,na.rm=T),col="red")
# 		abline(v=quantile(dem.stats[,j,i],p=0.975,na.rm=T),col="red")
# 	}
# 	
# 	hist(boot.ESS[,i],col="grey",xlab=main.titles[4],main="")
# 	abline(v=mean(boot.ESS[,i],na.rm=T))
# 	abline(v=median(boot.ESS[,i],na.rm=T),col="blue")
# 	abline(v=quantile(boot.ESS[,i],p=0.025,na.rm=T),col="red")
# 	abline(v=quantile(boot.ESS[,i],p=0.975,na.rm=T),col="red")
# 	abline(v=f.intercepts[i],col="green",lwd=2)
# }
# 
# lam.diff=dem.stats[,1,1]-dem.stats[,1,2]
# mean(lam.diff,na.rm=T)
# quantile(lam.diff,p=c(0.025,0.5,0.975),na.rm=T)
# R0.diff=dem.stats[,2,1]-dem.stats[,2,2]
# mean(R0.diff,na.rm=T)
# quantile(R0.diff,p=c(0.025,0.5,0.975),na.rm=T)
# T.diff=dem.stats[,3,1]-dem.stats[,3,2]
# mean(T.diff,na.rm=T)
# quantile(T.diff,p=c(0.025,0.5,0.975),na.rm=T)
# 
# #save(dem.stats,boot.ESS,file="c:\\temp\\ipm sheffield\\bootstrap samples.Rdata")

```

